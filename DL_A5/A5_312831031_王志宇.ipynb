{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import Transformer\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "import warnings\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "import yaml\n",
    "import json\n",
    "import os\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class index2char():\n",
    "    def __init__(self, root, tokenizer=None):\n",
    "        if tokenizer is None:\n",
    "            with open(root + '/tokenizer.yaml', 'r') as f:\n",
    "                self.tokenizer = yaml.load(f, Loader=yaml.CLoader)\n",
    "        else:\n",
    "            self.tokenizer = tokenizer\n",
    "    \n",
    "    def __call__(self, indices:list, without_token=True):\n",
    "        if type(indices) == Tensor:\n",
    "            indices = indices.tolist()\n",
    "        result = ''.join([self.tokenizer['index_2_char'][i] for i in indices])\n",
    "        if without_token:\n",
    "            result = result.split('[eos]')[0]\n",
    "            result = result.replace('[sos]', '').replace('[eos]', '').replace('[pad]', '')\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(pred:list, target:list) -> float:\n",
    "    \"\"\"\n",
    "    pred: list of strings\n",
    "    target: list of strings\n",
    "\n",
    "    return: accuracy(%)\n",
    "    \"\"\"\n",
    "    if len(pred) != len(target):\n",
    "        raise ValueError('length of pred and target must be the same')\n",
    "    correct = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == target[i]:\n",
    "            correct += 1\n",
    "    return correct / len(pred) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_num = 31\n",
    "embedding_dim = 512\n",
    "num_layers = 8\n",
    "num_heads = 8\n",
    "ff_dim = 1024\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellCorrectionDataset(Dataset):\n",
    "    def __init__(self, root, split:str = 'train', tokenizer=None, padding:int =0):\n",
    "        super(SpellCorrectionDataset, self).__init__()\n",
    "        #load your data here\n",
    "        self.padding = padding\n",
    "        \n",
    "        if tokenizer:\n",
    "            self.tokenizer = tokenizer\n",
    "        else:\n",
    "            with open(os.path.join(root, 'tokenizer.yaml'), 'r') as f:\n",
    "                self.tokenizer = yaml.load(f, Loader=yaml.CLoader)\n",
    "        \n",
    "        data_path = os.path.join(root, f'{split}.json')\n",
    "        with open(data_path, 'r') as f:\n",
    "            self.all_data = json.load(f)\n",
    "        self.data =[]\n",
    "        for line in range(len(self.all_data)):\n",
    "            for input in self.all_data[line]['input']:\n",
    "                self.data.append({'input':input,'target':self.all_data[line]['target']})\n",
    "    \n",
    "    def tokenize(self, text:str):\n",
    "        # tokenize your text here\n",
    "        # ex: \"data\" -> [4, 1, 20, 1]\n",
    "        \n",
    "        # 將文本轉換為索引序列\n",
    "        tokens = [self.tokenizer['char_2_index'].get(char, 0) for char in text]  # 0 可以是未識別字符的索引\n",
    "        # 根據指定的padding進行填充或截斷\n",
    "        if self.padding > 0:\n",
    "            tokens = tokens[:self.padding] + [0] * max(0, self.padding - len(tokens))  # 使用 0 進行填充\n",
    "        return tokens\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # get your data by index here\n",
    "        # ex: return input_ids, target_ids\n",
    "        # return type: torch.tensor\n",
    "        item = self.data[index]\n",
    "        input_text = item['input']\n",
    "        input_ids = self.tokenize(input_text)\n",
    "        target_text = item['target']\n",
    "        target_ids = self.tokenize(target_text)\n",
    "        # print(f\"input_ids={input_ids}\")\n",
    "        # print(f\"input_text={input_text}\")\n",
    "        # print(f\"target_text={target_text}\")\n",
    "        # print(f\"target_ids={target_ids}\")\n",
    "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000, batch_first: bool = False):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.batch_first:\n",
    "            x = x.transpose(0, 1)\n",
    "            # print(f\"x shape: {x.shape}, pe shape: {self.pe[:x.size(0)].shape}\")\n",
    "            x = x + self.pe[:x.size(0)]\n",
    "            return self.dropout(x.transpose(0, 1))\n",
    "        else:\n",
    "            x = x + self.pe[:x.size(0)]\n",
    "            return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_emb, hid_dim, n_layers, n_heads, ff_dim, dropout, max_length=100):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.tok_embedding = nn.Embedding(num_emb, hid_dim)\n",
    "        self.pos_embedding = PositionalEncoding(hid_dim, dropout, max_length, batch_first=True)\n",
    "        # self.layer = <nn.TransformerEncoderLayer>\n",
    "        # self.encoder = <nn.TransformerEncoder>\n",
    "        self.layer = nn.TransformerEncoderLayer(d_model=hid_dim, nhead=n_heads, dim_feedforward=ff_dim, dropout=dropout)\n",
    "        self.encoder = nn.TransformerEncoder(self.layer, num_layers=n_layers)\n",
    "        \n",
    "\n",
    "    def forward(self, src, src_mask=None, src_pad_mask=None):\n",
    "        # tgt = your_embeddings(?)\n",
    "        src_emb = self.tok_embedding(src)\n",
    "        src_emb = self.pos_embedding(src_emb)\n",
    "        # src = self.encoder(?)\n",
    "        enc_src = self.encoder(src_emb, mask=src_mask, src_key_padding_mask=src_pad_mask)\n",
    "        \n",
    "        return enc_src\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_emb, hid_dim, n_layers, n_heads, ff_dim, dropout, max_length=100):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tok_embedding = nn.Embedding(num_emb, hid_dim)\n",
    "        self.pos_embedding = PositionalEncoding(hid_dim, dropout, max_length, batch_first=True)\n",
    "        # self.layer = <nn.TransformerDecoderLayer>\n",
    "        # self.encoder = <nn.TransformerDecoder>\n",
    "        self.layer = nn.TransformerDecoderLayer(d_model=hid_dim, nhead=n_heads, dim_feedforward=ff_dim, dropout=dropout)\n",
    "        self.decoder = nn.TransformerDecoder(self.layer, num_layers=n_layers)\n",
    "\n",
    "    def forward(self, tgt, enc_src, tgt_mask=None, memory_mask=None, src_pad_mask=None, tgt_key_padding_mask=None):\n",
    "        # tgt = your_embeddings(?)\n",
    "        tgt_emb = self.tok_embedding(tgt)\n",
    "        tgt_emb = self.pos_embedding(tgt_emb)\n",
    "        \n",
    "        # tgt = self.decoder(?)\n",
    "        dec_output = self.decoder(tgt_emb, enc_src, tgt_mask=tgt_mask,\n",
    "                                  memory_mask=memory_mask,\n",
    "                                  tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                  memory_key_padding_mask=src_pad_mask)\n",
    "        \n",
    "        return dec_output\n",
    "\n",
    "class TransformerAutoEncoder(nn.Module):\n",
    "    def __init__(self, num_emb, hid_dim, n_layers, n_heads, ff_dim, dropout, max_length=100, encoder=None):\n",
    "        super(TransformerAutoEncoder, self).__init__()\n",
    "        if encoder is None:\n",
    "            self.encoder = Encoder(num_emb, hid_dim, n_layers, n_heads, ff_dim, dropout, max_length)\n",
    "        else:\n",
    "            self.encoder = encoder\n",
    "        self.decoder = Decoder(num_emb, hid_dim, n_layers, n_heads, ff_dim, dropout, max_length)\n",
    "        self.output_layer = nn.Linear(hid_dim, num_emb)\n",
    "\n",
    "    def forward(self, src, tgt, src_pad_mask=None, tgt_mask=None, tgt_pad_mask=None):\n",
    "        # enc_src = self.encoder(?)\n",
    "        enc_src = self.encoder(src, src_pad_mask=src_pad_mask)\n",
    "        \n",
    "        # out = self.decoder(?)\n",
    "        dec_out = self.decoder(tgt, enc_src, src_pad_mask=src_pad_mask, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_pad_mask)  \n",
    "\n",
    "        out = self.output_layer(dec_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_padding_mask(src, pad_idx):\n",
    "    # detect where the padding value is\n",
    "    pad_mask = (src == pad_idx).transpose(0, 1)  # 生成布林掩碼，padding 位置為 True\n",
    "    return pad_mask\n",
    "\n",
    "def gen_mask(seq):\n",
    "    # triu mask for decoder\n",
    "    seq_len = seq.size(0)  # 獲取序列長度\n",
    "    # 生成上三角掩碼，並擴展以適配注意力的計算\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    return mask\n",
    "\n",
    "def get_index(pred, dim=1):\n",
    "    return pred.clone().argmax(dim=dim)\n",
    "\n",
    "def random_change_idx(data: torch.Tensor, prob: float = 0.2):\n",
    "    # randomly change the index of the input data\n",
    "    change_mask = torch.rand(data.size()) < prob  # 隨機生成的布林掩碼\n",
    "    new_values = torch.randint(0, data.size(-1), data.size(), dtype=torch.long)\n",
    "    # 用新值替換原始數據中的部分值\n",
    "    sample = data.clone()\n",
    "    sample[change_mask] = new_values[change_mask]\n",
    "    return sample\n",
    "\n",
    "def random_masked(data: torch.Tensor, prob: float = 0.2, mask_idx: int = 3):\n",
    "    # randomly mask the input data\n",
    "    mask = torch.rand(data.size()) < prob  # 隨機生成的布林掩碼\n",
    "    # 將選中的值替換為指定的 mask_idx\n",
    "    sample = data.clone()\n",
    "    sample[mask] = mask_idx\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained encoder with random mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nycuai334/cedric/.conda/lib/python3.11/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# You can try to pretrain the Encoder here!\n",
    "class PretrainedMaskedEncoder(Encoder):\n",
    "    def __init__(self, num_emb, hid_dim, n_layers, n_heads, ff_dim, dropout, max_length=100, pretrained=True):\n",
    "        super(PretrainedMaskedEncoder, self).__init__(num_emb, hid_dim, n_layers, n_heads, ff_dim, dropout, max_length)\n",
    "        self.pretrained = pretrained\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        # Apply random masking before passing through the encoder\n",
    "        if self.pretrained:\n",
    "            src = random_masked(src, prob=0.2, mask_idx=0)  # Apply random masking to input data\n",
    "        return super().forward(src, src_mask, src_key_padding_mask)\n",
    "\n",
    "encoder = PretrainedMaskedEncoder(embedding_num, embedding_dim, num_layers, num_heads, ff_dim, dropout,  pretrained=True)\n",
    "\n",
    "# Define some dummy input (batch_size=4, sequence_length=10)\n",
    "src = torch.randint(0, embedding_num, (4, 10))\n",
    "\n",
    "# Forward pass through the encoder\n",
    "output = encoder(src)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train our spelling correction transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "i2c = index2char('./data/')\n",
    "\n",
    "trainset = SpellCorrectionDataset('./data/', padding=22)\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "testset = SpellCorrectionDataset('./data/', split='new_test', padding=22)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=False)\n",
    "valset = SpellCorrectionDataset('./data/', split='test', padding=22)\n",
    "valloader = DataLoader(valset, batch_size=32, shuffle=False)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(dataloader, model, device, logout=False):\n",
    "    pred_str_list = []\n",
    "    tgt_str_list = []\n",
    "    input_str_list = []\n",
    "    losses = []\n",
    "    for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = torch.full((tgt.shape[0], tgt.shape[1]), fill_value=0, device=device)\n",
    "            tgt_input[:, 0] = 1  # Set first token to <sos>\n",
    "            \n",
    "            for i in range(tgt.shape[1]-1):\n",
    "                # Generate the padding masks\n",
    "                src_pad_mask = gen_padding_mask(src, pad_idx=0)\n",
    "                tgt_pad_mask = gen_padding_mask(tgt_input, pad_idx=0)\n",
    "                tgt_mask = gen_mask(tgt_input).to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                pred = model(src, tgt_input,\n",
    "                             src_pad_mask=src_pad_mask,\n",
    "                             tgt_mask=tgt_mask,\n",
    "                             tgt_pad_mask=tgt_pad_mask)\n",
    "                \n",
    "                # pred = <get the prediction idx from the model>\n",
    "                # assign the prediction idx to the next token of tgt_input\n",
    "                pred_idx = get_index(pred[:, -1, :])\n",
    "                tgt_input[:, i + 1] = pred_idx  # 將預測結果添加到 tgt_input\n",
    "                \n",
    "            for i in range (tgt.shape[0]):\n",
    "                pred_str_list.append(i2c(tgt_input[i].tolist()))\n",
    "                tgt_str_list.append(i2c(tgt[i].tolist()))\n",
    "                input_str_list.append(i2c(src[i].tolist()))\n",
    "                if logout:\n",
    "                    print('='*30)\n",
    "                    print(f'input: {input_str_list[-1]}')\n",
    "                    print(f'pred: {pred_str_list[-1]}')\n",
    "                    print(f'target: {tgt_str_list[-1]}')\n",
    "            loss = ce_loss(pred[:, :-1, :].permute(0, 2, 1), tgt[:, 1:])\n",
    "            losses.append(loss.item())\n",
    "    print(f\"test_acc: {metrics(pred_str_list, tgt_str_list):.2f}\", f\"test_loss: {sum(losses)/len(losses):.2f}\", end=' | ')\n",
    "    print(f\"[pred: {pred_str_list[0]} target: {tgt_str_list[0]}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:   1%|          | 3/404 [00:00<00:15, 26.53iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:   2%|▏         | 9/404 [00:00<00:14, 26.86iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:   4%|▍         | 18/404 [00:00<00:13, 28.10iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:   5%|▌         | 22/404 [00:00<00:13, 28.98iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:   8%|▊         | 31/404 [00:01<00:12, 28.76iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:   8%|▊         | 34/404 [00:01<00:13, 28.23iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  10%|▉         | 40/404 [00:01<00:13, 27.56iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  12%|█▏        | 49/404 [00:01<00:12, 28.19iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  13%|█▎        | 52/404 [00:01<00:12, 27.94iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  14%|█▍        | 58/404 [00:02<00:12, 27.80iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  16%|█▌        | 65/404 [00:02<00:11, 28.80iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  18%|█▊        | 71/404 [00:02<00:11, 28.32iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  19%|█▉        | 77/404 [00:02<00:11, 28.02iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  21%|██        | 83/404 [00:03<00:11, 28.09iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  22%|██▏       | 89/404 [00:03<00:11, 28.15iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  24%|██▎       | 95/404 [00:03<00:11, 27.79iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  25%|██▌       | 101/404 [00:03<00:10, 27.71iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  26%|██▋       | 107/404 [00:03<00:10, 27.90iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  28%|██▊       | 113/404 [00:04<00:10, 27.60iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  29%|██▉       | 119/404 [00:04<00:10, 27.06iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  31%|███       | 125/404 [00:04<00:10, 27.09iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  32%|███▏      | 131/404 [00:04<00:10, 27.22iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  34%|███▍      | 137/404 [00:04<00:09, 27.25iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  35%|███▌      | 143/404 [00:05<00:09, 27.57iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  37%|███▋      | 149/404 [00:05<00:09, 27.45iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  39%|███▊      | 156/404 [00:05<00:08, 28.76iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  40%|████      | 162/404 [00:05<00:08, 27.90iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  42%|████▏     | 168/404 [00:06<00:08, 27.99iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  43%|████▎     | 174/404 [00:06<00:08, 28.21iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch0:  45%|████▍     | 180/404 [00:06<00:08, 27.83iter/s, loss: nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n",
      "pred shape: torch.Size([32, 22, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m ce_loss(pred[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m), tgt[:, \u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/cedric/.conda/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cedric/.conda/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cedric/.conda/lib/python3.11/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# encoder.pretrained_mode = False\n",
    "model = TransformerAutoEncoder(embedding_num, embedding_dim, num_layers, num_heads, ff_dim, dropout).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1) #choose your optimizer\n",
    "\n",
    "for eps in range(1000):\n",
    "    # train\n",
    "    losses = []\n",
    "    model.train()\n",
    "    i_bar = tqdm(trainloader, unit='iter', desc=f'epoch{eps}')\n",
    "    \n",
    "    for src, tgt in i_bar:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        # generate the mask and padding mask\n",
    "        src_pad_mask = gen_padding_mask(src, pad_idx=0)  # Generate the padding mask\n",
    "        tgt_pad_mask = gen_padding_mask(tgt, pad_idx=0)  # Generate the padding mask\n",
    "        tgt_mask = gen_mask(tgt).to(device)  # Generate the mask\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model(src, tgt,\n",
    "                     src_pad_mask=src_pad_mask,\n",
    "                     tgt_mask=tgt_mask,\n",
    "                     tgt_pad_mask=tgt_pad_mask)\n",
    "        \n",
    "        # print(f\"pred shape: {pred.shape}\")\n",
    "        \n",
    "        loss = ce_loss(pred[:, :-1, :].permute(0, 2, 1), tgt[:, 1:])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        i_bar.set_postfix_str(f\"loss: {sum(losses)/len(losses):.3f}\")\n",
    "        \n",
    "    # test\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        validation(testloader, model, device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        validation(valloader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(testloader, model, device, logout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(valloader, model, device, logout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
