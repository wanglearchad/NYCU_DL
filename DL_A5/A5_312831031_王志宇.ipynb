{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import Transformer\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "import warnings\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "import yaml\n",
    "import json\n",
    "import os\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class index2char():\n",
    "    def __init__(self, root, tokenizer=None):\n",
    "        if tokenizer is None:\n",
    "            with open(root + '/tokenizer.yaml', 'r') as f:\n",
    "                self.tokenizer = yaml.load(f, Loader=yaml.CLoader)\n",
    "        else:\n",
    "            self.tokenizer = tokenizer\n",
    "    \n",
    "    def __call__(self, indices:list, without_token=True):\n",
    "        if type(indices) == Tensor:\n",
    "            indices = indices.tolist()\n",
    "        result = ''.join([self.tokenizer['index_2_char'][i] for i in indices])\n",
    "        if without_token:\n",
    "            result = result.split('[eos]')[0]\n",
    "            result = result.replace('[sos]', '').replace('[eos]', '').replace('[pad]', '')\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(pred:list, target:list) -> float:\n",
    "    \"\"\"\n",
    "    pred: list of strings\n",
    "    target: list of strings\n",
    "\n",
    "    return: accuracy(%)\n",
    "    \"\"\"\n",
    "    if len(pred) != len(target):\n",
    "        raise ValueError('length of pred and target must be the same')\n",
    "    correct = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == target[i]:\n",
    "            correct += 1\n",
    "    return correct / len(pred) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_num = 31\n",
    "embedding_dim = 512\n",
    "num_layers = 8\n",
    "num_heads = 8\n",
    "ff_dim = 1024\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellCorrectionDataset(Dataset):\n",
    "    def __init__(self, root, split:str = 'train', tokenizer=None, padding:int =0):\n",
    "        super(SpellCorrectionDataset, self).__init__()\n",
    "        #load your data here\n",
    "        self.padding = padding\n",
    "        \n",
    "        if tokenizer:\n",
    "            self.tokenizer = tokenizer\n",
    "        else:\n",
    "            with open(os.path.join(root, 'tokenizer.yaml'), 'r') as f:\n",
    "                self.tokenizer = yaml.load(f, Loader=yaml.CLoader)\n",
    "        \n",
    "        data_path = os.path.join(root, f'{split}.json')\n",
    "        with open(data_path, 'r') as f:\n",
    "            self.all_data = json.load(f)\n",
    "        self.data =[]\n",
    "        for line in range(len(self.all_data)):\n",
    "            for input in self.all_data[line]['input']:\n",
    "                self.data.append({'input':input,'target':self.all_data[line]['target']})\n",
    "    \n",
    "    def tokenize(self, text:str):\n",
    "        # tokenize your text here\n",
    "        # ex: \"data\" -> [4, 1, 20, 1]\n",
    "        \n",
    "        # 將文本轉換為索引序列\n",
    "        tokens = [self.tokenizer['char_2_index'].get(char, 0) for char in text]  # 0 可以是未識別字符的索引\n",
    "        # 根據指定的padding進行填充或截斷\n",
    "        if self.padding > 0:\n",
    "            tokens = tokens[:self.padding] + [0] * max(0, self.padding - len(tokens))  # 使用 0 進行填充\n",
    "        return tokens\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # get your data by index here\n",
    "        # ex: return input_ids, target_ids\n",
    "        # return type: torch.tensor\n",
    "        item = self.data[index]\n",
    "        input_text = item['input']\n",
    "        input_ids = self.tokenize(input_text)\n",
    "        target_text = item['target']\n",
    "        target_ids = self.tokenize(target_text)\n",
    "        # print(f\"input_ids={input_ids}\")\n",
    "        # print(f\"input_text={input_text}\")\n",
    "        # print(f\"target_text={target_text}\")\n",
    "        # print(f\"target_ids={target_ids}\")\n",
    "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000, batch_first: bool = False):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.batch_first:\n",
    "            x = x.transpose(0, 1)\n",
    "            # print(f\"x shape: {x.shape}, pe shape: {self.pe[:x.size(0)].shape}\")\n",
    "            x = x + self.pe[:x.size(0)]\n",
    "            return self.dropout(x.transpose(0, 1))\n",
    "        else:\n",
    "            x = x + self.pe[:x.size(0)]\n",
    "            return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_emb, hid_dim, n_layers, n_heads, ff_dim, dropout, max_length=100):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.tok_embedding = nn.Embedding(num_emb, hid_dim)\n",
    "        self.pos_embedding = PositionalEncoding(hid_dim, dropout, max_length, batch_first=True)\n",
    "        # self.layer = <nn.TransformerEncoderLayer>\n",
    "        # self.encoder = <nn.TransformerEncoder>\n",
    "        self.layer = nn.TransformerEncoderLayer(d_model=hid_dim, nhead=n_heads, dim_feedforward=ff_dim, dropout=dropout)\n",
    "        self.encoder = nn.TransformerEncoder(self.layer, num_layers=n_layers)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_pad_mask=None):\n",
    "        # tgt = your_embeddings(?)\n",
    "        src_emb = self.tok_embedding(src)\n",
    "        src_emb = self.pos_embedding(src_emb)\n",
    "        # print(f\"src_emb contains NaN: {torch.isnan(src_emb).any()}\")\n",
    "    \n",
    "        # src = self.encoder(?)\n",
    "        enc_src = self.encoder(src_emb, mask=src_mask, src_key_padding_mask=src_pad_mask) #這邊開始出現 nan\n",
    "        # print(f\"enc_src contains NaN: {torch.isnan(enc_src).any()}\")\n",
    "        \n",
    "        return enc_src\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_emb, hid_dim, n_layers, n_heads, ff_dim, dropout, max_length=100):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tok_embedding = nn.Embedding(num_emb, hid_dim)\n",
    "        nn.init.xavier_uniform_(self.tok_embedding.weight)\n",
    "        self.pos_embedding = PositionalEncoding(hid_dim, dropout, max_length, batch_first=True)\n",
    "        # self.layer = <nn.TransformerDecoderLayer>\n",
    "        # self.encoder = <nn.TransformerDecoder>\n",
    "        self.layer = nn.TransformerDecoderLayer(d_model=hid_dim, nhead=n_heads, dim_feedforward=ff_dim, dropout=dropout)\n",
    "        self.decoder = nn.TransformerDecoder(self.layer, num_layers=n_layers)\n",
    "\n",
    "    def forward(self, tgt, enc_src, tgt_mask=None, memory_mask=None, src_pad_mask=None, tgt_key_padding_mask=None):\n",
    "        # tgt = your_embeddings(?)\n",
    "        tgt_emb = self.tok_embedding(tgt)\n",
    "        tgt_emb = self.pos_embedding(tgt_emb)\n",
    "        \n",
    "        # tgt = self.decoder(?)\n",
    "        dec_output = self.decoder(tgt_emb, enc_src, tgt_mask=tgt_mask,\n",
    "                                  memory_mask=memory_mask,\n",
    "                                  tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                  memory_key_padding_mask=src_pad_mask)\n",
    "        \n",
    "        return dec_output\n",
    "\n",
    "class TransformerAutoEncoder(nn.Module):\n",
    "    def __init__(self, num_emb, hid_dim, n_layers, n_heads, ff_dim, dropout, max_length=100, encoder=None):\n",
    "        super(TransformerAutoEncoder, self).__init__()\n",
    "        if encoder is None:\n",
    "            self.encoder = Encoder(num_emb, hid_dim, n_layers, n_heads, ff_dim, dropout, max_length)\n",
    "        else:\n",
    "            self.encoder = encoder\n",
    "        self.decoder = Decoder(num_emb, hid_dim, n_layers, n_heads, ff_dim, dropout, max_length)\n",
    "        self.output_layer = nn.Linear(hid_dim, num_emb)\n",
    "\n",
    "    def forward(self, src, tgt, src_pad_mask=None, tgt_mask=None, tgt_pad_mask=None):\n",
    "        # enc_src = self.encoder(?)\n",
    "        enc_src = self.encoder(src, src_pad_mask=src_pad_mask)\n",
    "        # print(f\"enc_src:{enc_src}\")\n",
    "        \n",
    "        # out = self.decoder(?)\n",
    "        dec_out = self.decoder(tgt, enc_src, src_pad_mask=src_pad_mask, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_pad_mask)  \n",
    "        # print(f\"dec_out:{dec_out}\")\n",
    "        \n",
    "        out = self.output_layer(dec_out)\n",
    "        out = F.softmax(out, dim=-1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_padding_mask(src, pad_idx):\n",
    "    # detect where the padding value is\n",
    "    pad_mask = (src == pad_idx).transpose(0, 1)  # 生成布林掩碼，padding 位置為 True\n",
    "    return pad_mask\n",
    "\n",
    "def gen_mask(seq):\n",
    "    # triu mask for decoder\n",
    "    seq_len = seq.size(0)  # 獲取序列長度\n",
    "    # 生成上三角掩碼，並擴展以適配注意力的計算\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    return mask\n",
    "\n",
    "def get_index(pred, dim=1):\n",
    "    return pred.clone().argmax(dim=dim)\n",
    "\n",
    "def random_change_idx(data: torch.Tensor, prob: float = 0.2):\n",
    "    # randomly change the index of the input data\n",
    "    change_mask = torch.rand(data.size()) < prob  # 隨機生成的布林掩碼\n",
    "    new_values = torch.randint(0, data.size(-1), data.size(), dtype=torch.long)\n",
    "    # 用新值替換原始數據中的部分值\n",
    "    sample = data.clone()\n",
    "    sample[change_mask] = new_values[change_mask]\n",
    "    return sample\n",
    "\n",
    "def random_masked(data: torch.Tensor, prob: float = 0.2, mask_idx: int = 3):\n",
    "    # randomly mask the input data\n",
    "    mask = torch.rand(data.size()) < prob  # 隨機生成的布林掩碼\n",
    "    # 將選中的值替換為指定的 mask_idx\n",
    "    sample = data.clone()\n",
    "    sample[mask] = mask_idx\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained encoder with random mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# You can try to pretrain the Encoder here!\n",
    "class PretrainedMaskedEncoder(Encoder):\n",
    "    def __init__(self, num_emb, hid_dim, n_layers, n_heads, ff_dim, dropout, max_length=100, pretrained=True):\n",
    "        super(PretrainedMaskedEncoder, self).__init__(num_emb, hid_dim, n_layers, n_heads, ff_dim, dropout, max_length)\n",
    "        self.pretrained = pretrained\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        # Apply random masking before passing through the encoder\n",
    "        if self.pretrained:\n",
    "            src = random_masked(src, prob=0.2, mask_idx=0)  # Apply random masking to input data\n",
    "        return super().forward(src, src_mask, src_key_padding_mask)\n",
    "\n",
    "encoder = PretrainedMaskedEncoder(embedding_num, embedding_dim, num_layers, num_heads, ff_dim, dropout,  pretrained=True)\n",
    "\n",
    "# Define some dummy input (batch_size=4, sequence_length=10)\n",
    "src = torch.randint(0, embedding_num, (4, 10))\n",
    "\n",
    "# Forward pass through the encoder\n",
    "output = encoder(src)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train our spelling correction transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "i2c = index2char('./data/')\n",
    "\n",
    "trainset = SpellCorrectionDataset('./data/', padding=22)\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "testset = SpellCorrectionDataset('./data/', split='new_test', padding=22)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=False)\n",
    "valset = SpellCorrectionDataset('./data/', split='test', padding=22)\n",
    "valloader = DataLoader(valset, batch_size=32, shuffle=False)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(dataloader, model, device, logout=False):\n",
    "    pred_str_list = []\n",
    "    tgt_str_list = []\n",
    "    input_str_list = []\n",
    "    losses = []\n",
    "    for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = torch.full((tgt.shape[0], tgt.shape[1]), fill_value=0, device=device)\n",
    "            tgt_input[:, 0] = 1  # Set first token to <sos>\n",
    "            \n",
    "            for i in range(tgt.shape[1]-1):\n",
    "                # Generate the padding masks\n",
    "                src_pad_mask = gen_padding_mask(src, pad_idx=0)\n",
    "                tgt_pad_mask = gen_padding_mask(tgt_input, pad_idx=0)\n",
    "                tgt_mask = gen_mask(tgt_input).to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                pred = model(src, tgt_input,\n",
    "                             src_pad_mask=src_pad_mask,\n",
    "                             tgt_mask=tgt_mask,\n",
    "                             tgt_pad_mask=tgt_pad_mask)\n",
    "                \n",
    "                # pred = <get the prediction idx from the model>\n",
    "                # assign the prediction idx to the next token of tgt_input\n",
    "                pred_idx = get_index(pred[:, -1, :])\n",
    "                tgt_input[:, i + 1] = pred_idx  # 將預測結果添加到 tgt_input\n",
    "                \n",
    "            for i in range (tgt.shape[0]):\n",
    "                pred_str_list.append(i2c(tgt_input[i].tolist()))\n",
    "                tgt_str_list.append(i2c(tgt[i].tolist()))\n",
    "                input_str_list.append(i2c(src[i].tolist()))\n",
    "                if logout:\n",
    "                    print('='*30)\n",
    "                    print(f'input: {input_str_list[-1]}')\n",
    "                    print(f'pred: {pred_str_list[-1]}')\n",
    "                    print(f'target: {tgt_str_list[-1]}')\n",
    "            loss = ce_loss(pred[:, :-1, :].permute(0, 2, 1), tgt[:, 1:])\n",
    "            losses.append(loss.item())\n",
    "    print(f\"test_acc: {metrics(pred_str_list, tgt_str_list):.2f}\", f\"test_loss: {sum(losses)/len(losses):.2f}\", end=' | ')\n",
    "    print(f\"[pred: {pred_str_list[0]} target: {tgt_str_list[0]}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch_1:   1%|          | 4/404 [00:00<00:35, 11.24iter/s, loss: nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.00 test_loss: nan | [pred:  target: appreciate]\n",
      "test_acc: 0.00 test_loss: nan | [pred:  target: contented]\n"
     ]
    }
   ],
   "source": [
    "# encoder.pretrained_mode = False\n",
    "model = TransformerAutoEncoder(embedding_num, embedding_dim, num_layers, num_heads, ff_dim, dropout).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4) #choose your optimizer\n",
    "\n",
    "for epoch in range(1000):\n",
    "    # train\n",
    "    losses = []\n",
    "    model.train()\n",
    "    i_bar = tqdm(trainloader, unit='iter', desc=f'epoch_{epoch+1}')\n",
    "\n",
    "    for src, tgt in i_bar:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        # generate the mask and padding mask\n",
    "        src_pad_mask = gen_padding_mask(src, pad_idx=0)  # Generate the padding mask\n",
    "        tgt_pad_mask = gen_padding_mask(tgt, pad_idx=0)  # Generate the padding mask\n",
    "        tgt_mask = gen_mask(tgt).to(device)  # Generate the mask\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model(src, tgt,\n",
    "                     src_pad_mask=src_pad_mask,\n",
    "                     tgt_mask=tgt_mask,\n",
    "                     tgt_pad_mask=tgt_pad_mask)\n",
    "\n",
    "        loss = ce_loss(pred[:, :-1, :].permute(0, 2, 1), tgt[:, 1:])\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        i_bar.set_postfix_str(f\"loss: {sum(losses)/len(losses):.3f}\")\n",
    "        \n",
    "    # test\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        validation(testloader, model, device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        validation(valloader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "input: apreciate\n",
      "pred: \n",
      "target: appreciate\n",
      "==============================\n",
      "input: appeciate\n",
      "pred: \n",
      "target: appreciate\n",
      "==============================\n",
      "input: apprciate\n",
      "pred: \n",
      "target: appreciate\n",
      "==============================\n",
      "input: apprecate\n",
      "pred: \n",
      "target: appreciate\n",
      "==============================\n",
      "input: apprecite\n",
      "pred: \n",
      "target: appreciate\n",
      "==============================\n",
      "input: luve\n",
      "pred: \n",
      "target: love\n",
      "==============================\n",
      "input: culd\n",
      "pred: \n",
      "target: cold\n",
      "==============================\n",
      "input: heart\n",
      "pred: \n",
      "target: heart\n",
      "==============================\n",
      "input: televiseon\n",
      "pred: \n",
      "target: television\n",
      "==============================\n",
      "input: thone\n",
      "pred: \n",
      "target: phone\n",
      "==============================\n",
      "input: phace\n",
      "pred: \n",
      "target: phase\n",
      "==============================\n",
      "input: poam\n",
      "pred: \n",
      "target: poem\n",
      "==============================\n",
      "input: tomorraw\n",
      "pred: \n",
      "target: tomorrow\n",
      "==============================\n",
      "input: presishan\n",
      "pred: \n",
      "target: precision\n",
      "==============================\n",
      "input: presishion\n",
      "pred: \n",
      "target: precision\n",
      "==============================\n",
      "input: presisian\n",
      "pred: \n",
      "target: precision\n",
      "==============================\n",
      "input: presistion\n",
      "pred: \n",
      "target: precision\n",
      "==============================\n",
      "input: perver\n",
      "pred: \n",
      "target: prefer\n",
      "==============================\n",
      "input: predgudice\n",
      "pred: \n",
      "target: prejudice\n",
      "==============================\n",
      "input: predgudis\n",
      "pred: \n",
      "target: prejudice\n",
      "==============================\n",
      "input: recievor\n",
      "pred: \n",
      "target: receiver\n",
      "==============================\n",
      "input: reciover\n",
      "pred: \n",
      "target: receiver\n",
      "==============================\n",
      "input: relieve\n",
      "pred: \n",
      "target: relief\n",
      "==============================\n",
      "input: togather\n",
      "pred: \n",
      "target: together\n",
      "==============================\n",
      "input: remuttance\n",
      "pred: \n",
      "target: remittance\n",
      "==============================\n",
      "input: deposite\n",
      "pred: \n",
      "target: deposit\n",
      "==============================\n",
      "input: deposittt\n",
      "pred: \n",
      "target: deposit\n",
      "==============================\n",
      "input: peper\n",
      "pred: \n",
      "target: pepper\n",
      "==============================\n",
      "input: pepperrr\n",
      "pred: \n",
      "target: pepper\n",
      "==============================\n",
      "input: employe\n",
      "pred: \n",
      "target: employee\n",
      "==============================\n",
      "input: employezz\n",
      "pred: \n",
      "target: employee\n",
      "==============================\n",
      "input: beest\n",
      "pred: \n",
      "target: best\n",
      "==============================\n",
      "input: bestt\n",
      "pred: \n",
      "target: best\n",
      "==============================\n",
      "input: aset\n",
      "pred: \n",
      "target: best\n",
      "==============================\n",
      "input: feeture\n",
      "pred: \n",
      "target: feature\n",
      "==============================\n",
      "input: faeture\n",
      "pred: \n",
      "target: feature\n",
      "==============================\n",
      "input: featture\n",
      "pred: \n",
      "target: feature\n",
      "==============================\n",
      "input: gorges\n",
      "pred: \n",
      "target: gorgeous\n",
      "==============================\n",
      "input: gorgeus\n",
      "pred: \n",
      "target: gorgeous\n",
      "==============================\n",
      "input: gourgace\n",
      "pred: \n",
      "target: gorgeous\n",
      "==============================\n",
      "input: gripe\n",
      "pred: \n",
      "target: grip\n",
      "==============================\n",
      "input: hienous\n",
      "pred: \n",
      "target: heinous\n",
      "==============================\n",
      "input: hurple\n",
      "pred: \n",
      "target: purple\n",
      "==============================\n",
      "input: occassional\n",
      "pred: \n",
      "target: occasional\n",
      "==============================\n",
      "input: tirumph\n",
      "pred: \n",
      "target: triumph\n",
      "==============================\n",
      "input: triam\n",
      "pred: \n",
      "target: triumph\n",
      "==============================\n",
      "input: unforgatealbe\n",
      "pred: \n",
      "target: unforgettable\n",
      "==============================\n",
      "input: unforgattable\n",
      "pred: \n",
      "target: unforgettable\n",
      "==============================\n",
      "input: vesiable\n",
      "pred: \n",
      "target: visible\n",
      "==============================\n",
      "input: visable\n",
      "pred: \n",
      "target: visible\n",
      "test_acc: 0.00 test_loss: nan | [pred:  target: appreciate]\n"
     ]
    }
   ],
   "source": [
    "validation(testloader, model, device, logout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "input: contenpted\n",
      "pred: \n",
      "target: contented\n",
      "==============================\n",
      "input: begining\n",
      "pred: \n",
      "target: beginning\n",
      "==============================\n",
      "input: problam\n",
      "pred: \n",
      "target: problem\n",
      "==============================\n",
      "input: dirven\n",
      "pred: \n",
      "target: driven\n",
      "==============================\n",
      "input: ecstacy\n",
      "pred: \n",
      "target: ecstasy\n",
      "==============================\n",
      "input: juce\n",
      "pred: \n",
      "target: juice\n",
      "==============================\n",
      "input: localy\n",
      "pred: \n",
      "target: locally\n",
      "==============================\n",
      "input: compair\n",
      "pred: \n",
      "target: compare\n",
      "==============================\n",
      "input: pronounciation\n",
      "pred: \n",
      "target: pronunciation\n",
      "==============================\n",
      "input: transportibility\n",
      "pred: \n",
      "target: transportability\n",
      "==============================\n",
      "input: miniscule\n",
      "pred: \n",
      "target: minuscule\n",
      "==============================\n",
      "input: independant\n",
      "pred: \n",
      "target: independent\n",
      "==============================\n",
      "input: aranged\n",
      "pred: \n",
      "target: arranged\n",
      "==============================\n",
      "input: poartry\n",
      "pred: \n",
      "target: poetry\n",
      "==============================\n",
      "input: leval\n",
      "pred: \n",
      "target: level\n",
      "==============================\n",
      "input: basicaly\n",
      "pred: \n",
      "target: basically\n",
      "==============================\n",
      "input: triangulaur\n",
      "pred: \n",
      "target: triangular\n",
      "==============================\n",
      "input: unexpcted\n",
      "pred: \n",
      "target: unexpected\n",
      "==============================\n",
      "input: stanerdizing\n",
      "pred: \n",
      "target: standardizing\n",
      "==============================\n",
      "input: varable\n",
      "pred: \n",
      "target: variable\n",
      "==============================\n",
      "input: neigbours\n",
      "pred: \n",
      "target: neighbours\n",
      "==============================\n",
      "input: enxt\n",
      "pred: \n",
      "target: next\n",
      "==============================\n",
      "input: powerfull\n",
      "pred: \n",
      "target: powerful\n",
      "==============================\n",
      "input: practial\n",
      "pred: \n",
      "target: practical\n",
      "==============================\n",
      "input: repatition\n",
      "pred: \n",
      "target: repartition\n",
      "==============================\n",
      "input: repentence\n",
      "pred: \n",
      "target: repentance\n",
      "==============================\n",
      "input: substracts\n",
      "pred: \n",
      "target: subtracts\n",
      "==============================\n",
      "input: beed\n",
      "pred: \n",
      "target: bead\n",
      "==============================\n",
      "input: beame\n",
      "pred: \n",
      "target: beam\n",
      "==============================\n",
      "input: decieve\n",
      "pred: \n",
      "target: deceive\n",
      "==============================\n",
      "input: decant\n",
      "pred: \n",
      "target: decent\n",
      "==============================\n",
      "input: dag\n",
      "pred: \n",
      "target: dog\n",
      "==============================\n",
      "input: daing\n",
      "pred: \n",
      "target: doing\n",
      "==============================\n",
      "input: expence\n",
      "pred: \n",
      "target: expense\n",
      "==============================\n",
      "input: feirce\n",
      "pred: \n",
      "target: fierce\n",
      "==============================\n",
      "input: firery\n",
      "pred: \n",
      "target: fiery\n",
      "==============================\n",
      "input: fought\n",
      "pred: \n",
      "target: fort\n",
      "==============================\n",
      "input: fourth\n",
      "pred: \n",
      "target: forth\n",
      "==============================\n",
      "input: ham\n",
      "pred: \n",
      "target: harm\n",
      "==============================\n",
      "input: havest\n",
      "pred: \n",
      "target: harvest\n",
      "==============================\n",
      "input: immdiately\n",
      "pred: \n",
      "target: immediately\n",
      "==============================\n",
      "input: inehaustible\n",
      "pred: \n",
      "target: inexhaustible\n",
      "==============================\n",
      "input: journel\n",
      "pred: \n",
      "target: journal\n",
      "==============================\n",
      "input: leason\n",
      "pred: \n",
      "target: lesson\n",
      "==============================\n",
      "input: mantain\n",
      "pred: \n",
      "target: maintain\n",
      "==============================\n",
      "input: miricle\n",
      "pred: \n",
      "target: miracle\n",
      "==============================\n",
      "input: oportunity\n",
      "pred: \n",
      "target: opportunity\n",
      "==============================\n",
      "input: parenthasis\n",
      "pred: \n",
      "target: parenthesis\n",
      "==============================\n",
      "input: recetion\n",
      "pred: \n",
      "target: recession\n",
      "==============================\n",
      "input: scadual\n",
      "pred: \n",
      "target: schedule\n",
      "test_acc: 0.00 test_loss: nan | [pred:  target: contented]\n"
     ]
    }
   ],
   "source": [
    "validation(valloader, model, device, logout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
